{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wavenet-btc",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hoC9rlPPdIaF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_variable(name, shape, seed=None):\n",
        "    ''' Create variable with Xavier initialization '''\n",
        "    init = tf.contrib.layers.xavier_initializer(seed=seed)\n",
        "    return tf.get_variable(name=name, shape=shape, initializer=init)\n",
        "\n",
        "def create_bias_variable(name, shape):\n",
        "    ''' Create variable with zeros initialization '''\n",
        "    init = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
        "    return tf.get_variable(name=name, shape=shape, initializer=init)\n",
        "\n",
        "def time_to_batch(inputs, dilation):\n",
        "    ''' If necessary zero-pads inputs and reshape by dilation '''\n",
        "    with tf.variable_scope('time_to_batch'):\n",
        "        _, width, num_channels = inputs.get_shape().as_list()\n",
        "\n",
        "        width_pad = int(dilation * np.ceil((width + dilation) * 1.0 / dilation))\n",
        "        pad_left = width_pad - width\n",
        "\n",
        "        perm = (1, 0, 2)\n",
        "        shape = (int(width_pad / dilation), -1, num_channels)\n",
        "        padded = tf.pad(inputs, [[0, 0], [pad_left, 0], [0, 0]])\n",
        "        transposed = tf.transpose(padded, perm)\n",
        "        reshaped = tf.reshape(transposed, shape)\n",
        "        outputs = tf.transpose(reshaped, perm)\n",
        "        return outputs\n",
        "\n",
        "def batch_to_time(inputs, dilation, crop_left=0):\n",
        "    ''' Reshape to 1d signal, and remove excess zero-padding '''\n",
        "    with tf.variable_scope('batch_to_time'):\n",
        "        shape = tf.shape(inputs)\n",
        "        batch_size = shape[0] / dilation\n",
        "        width = shape[1]\n",
        "        \n",
        "        out_width = tf.to_int32(width * dilation)\n",
        "        _, _, num_channels = inputs.get_shape().as_list()\n",
        "        \n",
        "        perm = (1, 0, 2)\n",
        "        new_shape = (out_width, -1, num_channels) # missing dim: batch_size\n",
        "        transposed = tf.transpose(inputs, perm)    \n",
        "        reshaped = tf.reshape(transposed, new_shape)\n",
        "        outputs = tf.transpose(reshaped, perm)\n",
        "        cropped = tf.slice(outputs, [0, crop_left, 0], [-1, -1, -1])\n",
        "        return cropped\n",
        "\n",
        "def conv1d(inputs, out_channels, filter_width=2, stride=1, padding='VALID', \n",
        "        activation=tf.nn.relu, seed=None, bias=True, name='conv1d'):\n",
        "    ''' Normal 1D convolution operator ''' \n",
        "    with tf.variable_scope(name):\n",
        "        in_channels = inputs.get_shape().as_list()[-1]\n",
        "\n",
        "        W = create_variable('W', (filter_width, in_channels, out_channels), seed)\n",
        "\n",
        "        outputs = tf.nn.conv1d(inputs, W, stride=stride, padding=padding)\n",
        "\n",
        "        if bias:\n",
        "            b = create_bias_variable('bias', (out_channels, ))\n",
        "            outputs += tf.expand_dims(tf.expand_dims(b, 0), 0)\n",
        "\n",
        "        if activation:\n",
        "            outputs = activation(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "def dilated_conv(inputs, out_channels, filter_width=2, dilation=1, stride=1, \n",
        "        padding='VALID', name='dilated_conv', activation=tf.nn.relu, seed=None):\n",
        "    ''' Warpper for 1D convolution to include dilation '''\n",
        "    with tf.variable_scope(name):\n",
        "        width = inputs.get_shape().as_list()[1]\n",
        "\n",
        "        inputs_ = time_to_batch(inputs, dilation)\n",
        "        outputs_ = conv1d(inputs_, out_channels, filter_width, stride, padding, activation, seed)\n",
        "\n",
        "        out_width = outputs_.get_shape().as_list()[1] * dilation\n",
        "        diff = out_width - width\n",
        "        outputs = batch_to_time(outputs_, dilation, crop_left=diff)\n",
        "\n",
        "        # Add additional shape information.\n",
        "        tensor_shape = [tf.Dimension(None), tf.Dimension(width), tf.Dimension(out_channels)]\n",
        "        outputs.set_shape(tf.TensorShape(tensor_shape))\n",
        "\n",
        "        return outputs\n",
        "    \n",
        "class Model(object):\n",
        "\n",
        "    def __init__(self, **params):\n",
        "        self.num_time_steps = params.get('num_time_steps')\n",
        "        self.fields = params.get('fields')\n",
        "        self.num_filters = params.get('num_filters')\n",
        "        self.num_layers = params.get('num_layers')\n",
        "        self.learning_rate = params.get('learning_rate', 1e-3)\n",
        "        self.regularization = params.get('regularization', 1e-2)\n",
        "        self.n_iter = int(params.get('n_iter'))\n",
        "        self.logdir = params.get('logdir')\n",
        "        self.seed = params.get('seed', None)\n",
        "\n",
        "        assert self.num_layers >= 2, \"Must use at least 2 dilation layers\"\n",
        "\n",
        "        self._build_graph()\n",
        "        \n",
        "    def _build_graph(self):\n",
        "        tf.reset_default_graph()\n",
        "\n",
        "        self.inputs = dict()\n",
        "        self.targets = dict()\n",
        "\n",
        "        with tf.variable_scope('input'):\n",
        "            for f in self.fields:\n",
        "                self.inputs[f] = tf.placeholder(tf.float32, (None, self.num_time_steps), 'input_%s' % f)\n",
        "                self.targets[f] = tf.placeholder(tf.float32, (None, self.num_time_steps), 'target_%s' % f)\n",
        "        \n",
        "        # Create wavenet for each field being regressed\n",
        "        self.costs = dict()\n",
        "        self.optimizers = dict()\n",
        "        self.outputs = dict()\n",
        "        for field in self.fields:\n",
        "            with tf.variable_scope(field):\n",
        "\n",
        "                # Input layer with conditioning gates\n",
        "                conditions = list()\n",
        "                with tf.variable_scope('input_layer'):\n",
        "                    for k in self.inputs.keys():\n",
        "                        with tf.variable_scope('condition_%s' % k):\n",
        "                            dilation = 1\n",
        "                            X = tf.expand_dims(self.inputs[k], 2)\n",
        "                            h = dilated_conv(X, self.num_filters, name='input_conv_%s' % k, seed=self.seed)\n",
        "                            skip = conv1d(X, self.num_filters, filter_width=1, name='skip_%s' % k, \n",
        "                                    activation=None, seed=self.seed)\n",
        "                            conditions.append(h + skip)\n",
        "\n",
        "                    output = tf.add_n(conditions)\n",
        "\n",
        "                # Intermediate dilation layers\n",
        "                with tf.variable_scope('dilated_stack'):\n",
        "                    for i in range(self.num_layers - 1):\n",
        "                        with tf.variable_scope('layer_%d' % i):\n",
        "                            dilation = 2 ** (i + 1)\n",
        "                            h = dilated_conv(output, self.num_filters, dilation=dilation, name='dilated_conv', \n",
        "                                    seed=self.seed)\n",
        "                            output = h + output\n",
        "\n",
        "                # Output layer\n",
        "                with tf.variable_scope('output_layer'):\n",
        "                    output = conv1d(output, 1, filter_width=1, name='output_conv', activation=None,\n",
        "                            seed=self.seed)\n",
        "                    self.outputs[field] = tf.squeeze(output, [2])\n",
        "\n",
        "            # Optimization\n",
        "            with tf.variable_scope('optimize_%s' % field):\n",
        "                mae_cost = tf.reduce_mean(tf.losses.absolute_difference(\n",
        "                    labels=self.targets[field], predictions=self.outputs[field]))\n",
        "                trainable = tf.trainable_variables(scope=field)\n",
        "                l2_cost = tf.add_n([tf.nn.l2_loss(v) for v in trainable if not ('bias' in v.name)])\n",
        "                self.costs[field] = mae_cost + self.regularization / 2 * l2_cost\n",
        "                tf.summary.scalar('loss_%s' % field, self.costs[field])\n",
        "\n",
        "                self.optimizers[field] = tf.train.AdamOptimizer(self.learning_rate).minimize(self.costs[field])\n",
        "\n",
        "        # Tensorboard output\n",
        "        run_id = ''.join(random.choice(string.uppercase) for x in range(6))\n",
        "        self.run_dir = os.path.join(self.logdir, run_id)\n",
        "        self.writer = tf.summary.FileWriter(self.run_dir)\n",
        "        self.writer.add_graph(tf.get_default_graph())\n",
        "        self.run_metadata = tf.RunMetadata()\n",
        "        self.summaries = tf.summary.merge_all()\n",
        "\n",
        "        print(\"Graph for run %s created\" % run_id)\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.sess.close()\n",
        "\n",
        "    def train(self, targets, features):\n",
        "\n",
        "        saver = tf.train.Saver(var_list=tf.trainable_variables(), max_to_keep=1)\n",
        "        checkpoint_path = os.path.join(self.run_dir, 'model.ckpt')\n",
        "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "        print(\"Writing TensorBoard log to %s\" % self.run_dir)\n",
        "\n",
        "        # Sort input dictionaries into the feed dictionary\n",
        "        feed_dict = dict()\n",
        "        for field in self.fields:\n",
        "            feed_dict[self.inputs[field]] = features[field]\n",
        "            feed_dict[self.targets[field]] = targets[field]\n",
        "\n",
        "        for step in range(self.n_iter):\n",
        "            opts = [self.optimizers[f] for f in self.fields]\n",
        "            _ = self.sess.run(opts, feed_dict=feed_dict)\n",
        "\n",
        "            # Save summaries every 100 steps\n",
        "            if (step % 100) == 0:\n",
        "                summary = self.sess.run([self.summaries], feed_dict=feed_dict)[0]\n",
        "                self.writer.add_summary(summary, step)\n",
        "                self.writer.flush()\n",
        "\n",
        "            # Print cost to console every 1000 steps, also store metadata\n",
        "            if (step % 1000) == 0:\n",
        "                costs = [self.costs[f] for f in self.fields]\n",
        "                costs = self.sess.run(costs, feed_dict=feed_dict, \n",
        "                        run_metadata=self.run_metadata, options=run_options)\n",
        "                self.writer.add_run_metadata(self.run_metadata, 'step_%d' % step)\n",
        "\n",
        "                cost = \", \".join(map(lambda x: \"%.06f\" % x, costs))\n",
        "                print(\"Losses at step %d: %s\" % (step, cost))\n",
        "\n",
        "        costs = [self.costs[f] for f in self.fields]\n",
        "        costs = self.sess.run(costs, feed_dict=feed_dict)\n",
        "        cost = \", \".join(map(lambda x: \"%.06f\" % x, costs))\n",
        "        print(\"Final loss: %s\" % cost)\n",
        "\n",
        "        # Save final checkpoint of model\n",
        "        print(\"Storing model checkpoint %s\" % checkpoint_path)\n",
        "        saver.save(self.sess, checkpoint_path, global_step=step)\n",
        "\n",
        "        # Format output back into dictionary form\n",
        "        outputs = [self.outputs[f] for f in self.fields]\n",
        "        outputs = self.sess.run(outputs, feed_dict=feed_dict)\n",
        "\n",
        "        out_dict = dict()\n",
        "        for i, f in enumerate(self.fields):\n",
        "            out_dict[f] = outputs[i]\n",
        "\n",
        "        return out_dict\n",
        "        \n",
        "    def generate(self, num_steps, features):\n",
        "\n",
        "        forecast = dict()\n",
        "        for f in self.fields:\n",
        "            forecast[f] = list()\n",
        "\n",
        "        for step in range(num_steps):\n",
        "\n",
        "            feed_dict = dict()\n",
        "            for f in self.fields:\n",
        "                feed_dict[self.inputs[f]] = features[f]\n",
        "\n",
        "            outputs = [self.outputs[f] for f in self.fields]\n",
        "            outputs = self.sess.run(outputs, feed_dict=feed_dict)\n",
        "\n",
        "            for i, f in enumerate(self.fields):\n",
        "                features[f][0, :] = np.append(features[f][0, 1:], outputs[i][0, -1])\n",
        "                forecast[f].append(outputs[i][0, -1])\n",
        "        \n",
        "        for f in self.fields:\n",
        "            forecast[f] = np.array(forecast[f]).reshape(1, -1)\n",
        "\n",
        "        return forecast\n",
        "\n",
        "class Normalizer(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.norm_map = {}\n",
        "    \n",
        "    def fit(self, df):\n",
        "        for c in df.columns:\n",
        "            self.norm_map[c] = (df[c].mean(), df[c].std())\n",
        "    \n",
        "    def transform(self, df):\n",
        "        for c, (m, s) in self.norm_map.iteritems():\n",
        "            df.loc[:, c] = (df[c] - m) / s\n",
        "        return df\n",
        "\n",
        "    def undo_transform(self, df, suffix=None):\n",
        "        for c, (m, s) in self.norm_map.iteritems():\n",
        "            df.loc[:, c] = df[c] * s + m\n",
        "            if suffix is not None:\n",
        "                df.loc[:, c + suffix] = df[c + suffix] * s + m\n",
        "        return df\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_target_columns(train, test):\n",
        "        columns = train.columns.tolist()\n",
        "        train_t = train.copy()\n",
        "        test_t = test.copy()\n",
        "        for c in columns:\n",
        "            train_t.loc[:, c] = train[c].shift(-1)\n",
        "            train_t.loc[train_t.index.tolist()[-1], c] = test_t.loc[test_t.index.tolist()[0], c]\n",
        "            test_t.loc[:, c] = test[c].shift(-1)\n",
        "\n",
        "        return train, train_t, test.iloc[:-1,:], test_t.iloc[:-1,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bldgb2M5dPiW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = open('kaggle.json', 'w')\n",
        "f.write('{\"username\":\"username\",\"key\":\"key\"}')\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6D7teeVqdk2x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!pip install plotly\n",
        "!pip install mpl_finance\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DJLFT-Itdmis",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import os \n",
        "\n",
        "# if not os.path.isdir('/content/.kaggle/datasets/jessevent/all-crypto-currencies/'):\n",
        "#     print(\"file empty. download start\")\n",
        "#     !kaggle datasets download -d jessevent/all-crypto-currencies\n",
        "#     !unzip -o all-crypto-currencies.zip\n",
        "# else :\n",
        "#     print(\"file already downloads. file list : \")\n",
        "#     !ls /Users/bevislee/.kaggle/datasets/jessevent/all-crypto-currencies/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oi1qQ6eVmj-_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install mpld3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WUupqD-Fd9cc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import mpld3\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "pd.set_option('chained_assignment', None)\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "mpld3.enable_notebook()\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mmamo1ahlDZ0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.isdir('/Users/bevislee/.kaggle/datasets/mczielinski/bitcoin-historical-data'):\n",
        "    print(\"file empty. download start\")\n",
        "    !kaggle datasets download -d mczielinski/bitcoin-historical-data\n",
        "    !unzip -o bitcoin-historical-data.zip\n",
        "else :\n",
        "    print(\"file already downloads. file list : \")\n",
        "    !ls /Users/bevislee/.kaggle/datasets/mczielinski/bitcoin-historical-data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2rQuUSTkPP29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# coinbaseUSD = pd.read_csv('bitstampUSD_1-min_data_2012-01-01_to_2018-06-27.csv')\n",
        "# coinbaseUSD['Timestamp'] = pd.to_datetime(coinbaseUSD['Timestamp'], unit='s')\n",
        "# coinbaseUSD = coinbaseUSD[(coinbaseUSD.Timestamp > \"2017-10-19\")&(coinbaseUSD.Timestamp < \"2017-10-31\")]\n",
        "# coinbaseUSD['price'] = coinbaseUSD['Volume_(BTC)']*coinbaseUSD['Weighted_Price']\n",
        "\n",
        "# coinbaseUSD.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D4YOeGUplUMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# coinbase, coincheck, bitstamp 1분 단위 데이터 사용\n",
        "# 2017년 10월 19일 : 1일만 읽어서 1분 단위로 맞춰서 새로운 dataframe 생성\n",
        "\n",
        "# For intra-day trading, we only will work with one date at a time\n",
        "date = datetime(2017, 10, 19)\n",
        "\n",
        "# Names of files for each Bitcoin data set\n",
        "data_sets = {\n",
        "    'coinbase': 'coinbaseUSD_1-min_data_2014-12-01_to_2018-06-27.csv',\n",
        "    'bitstamp': 'bitstampUSD_1-min_data_2012-01-01_to_2018-06-27.csv'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h_3fBz2YldDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_data(path):\n",
        "    next_date = date + timedelta(days=1)\n",
        "    df = (\n",
        "        pd\n",
        "        .read_csv(path)\n",
        "        .assign(Timestamp = lambda x: pd.to_datetime(x.Timestamp, unit='s'))\n",
        "        .rename(columns={'Volume_(BTC)':'volume', 'Weighted_Price':'price'})\n",
        "        .loc[lambda x: (x.Timestamp >= date) & (x.Timestamp < next_date), ['Timestamp','price']]\n",
        "        .sort_values('Timestamp')\n",
        "        .set_index('Timestamp')\n",
        "    )\n",
        "    for c in df.columns:\n",
        "        df[c] = df[c].astype(float)\n",
        "    return df\n",
        "\n",
        "# Merge all three datasets together\n",
        "df = pd.concat([read_data(x) for x in data_sets.values()], axis=1)\n",
        "df.columns = ['%s_price' % x for x in data_sets.keys()]\n",
        "\n",
        "# Split into training/test - use last 6 hours of day for test\n",
        "train = df.iloc[:-240]\n",
        "test = df.iloc[-240:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8qTIV69tl6GW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yY7bqxRGmBUY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from model import Normalizer\n",
        "\n",
        "# Normalize data, create target/feature columns\n",
        "norm = Normalizer()\n",
        "norm.fit(train)\n",
        "\n",
        "input_columns = train.columns.tolist()\n",
        "\n",
        "train = norm.transform(train)\n",
        "test = norm.transform(test)\n",
        "\n",
        "train, train_target, test, test_target = norm.make_target_columns(train, test)\n",
        "\n",
        "print(train.shape[0])\n",
        "print(test.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2RU2DQkEo_eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4gwDepwXifWS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from model import Model\n",
        "\n",
        "# WaveNet params\n",
        "params = {\n",
        "    'num_time_steps': train.shape[0],\n",
        "    'num_filters': 1,\n",
        "    'num_layers': 7,\n",
        "    'learning_rate': 1e-3,\n",
        "    'regularization': 1e-2,\n",
        "    'n_iter': 15000,\n",
        "    'logdir': '/var/data/tensorboard',\n",
        "    'fields': input_columns,\n",
        "    'seed': 0\n",
        "}\n",
        "\n",
        "wavenet = Model(**params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cVSPRlSVifUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Format model input\n",
        "features = dict()\n",
        "targets = dict()\n",
        "for column in input_columns:\n",
        "    f = np.array(train[column])\n",
        "    f = np.reshape(f, (1, -1))\n",
        "    features[column] = f\n",
        "    \n",
        "    f = np.array(train_target[column])\n",
        "    f = np.reshape(f, (1, -1))\n",
        "    targets[column] = f\n",
        "\n",
        "print(\"feature\\n\",features['coinbase_price'])\n",
        "print(\"target\\n\",targets['coinbase_price'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2Rtl2ejpoJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run model\n",
        "with wavenet:\n",
        "    # Train\n",
        "    output = wavenet.train(targets, features)\n",
        "\n",
        "    # Generate\n",
        "    num_steps = test.shape[0]\n",
        "    pred = wavenet.generate(num_steps, features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "65LATjkTifQ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Merge outputs together\n",
        "for col in input_columns:\n",
        "    train_target[col + '_pred'] = np.reshape(output[col], (-1,))\n",
        "    test_target[col + '_pred'] = np.reshape(pred[col], (-1,))\n",
        "df = (\n",
        "    train_target\n",
        "    .append(test_target)\n",
        "    .pipe(lambda x: norm.undo_transform(x, suffix='_pred'))\n",
        ")\n",
        "\n",
        "for col in input_columns:\n",
        "    fig = plt.figure(figsize=(13, 9))\n",
        "    df[col].plot(label='Real', ls='--')\n",
        "    df[col + '_pred'].plot(label='Fit')\n",
        "    plt.title(col + \" on \" + date.strftime(\"%Y-%m-%d\"))\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f5MgzQrsifOE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7a314b2UifMG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "liXKX9ePifJO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hk3Qc_Q1gqYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V5tEelrqgjPY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}